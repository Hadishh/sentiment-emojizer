\documentclass[a4paper]{article} 
\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Hadi Sheikhi \hfill\\   
97521369\hfill\\
ha\_sheikhi@comp.iust.ac.ir
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Final Project Report\\ 
\normalsize 
Natural Language Processing\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------


%\blindtext
[TODO] Some explanation about the tasks and the project.
\section{Word2Vec algorithm}
By using the word2vec implementations of the CS224n course at Stanford University, after training the model for each label we get losses around 5. Now let's consider the two most common tokens between each label and see the similarity between the resulting vectors.\\
\begin{align*}
	\csvautotabular{../reports/word2vec/similarity.csv}
\end{align*} 
As the table shows many words have different word vectors in different classes! But if we take a look at for example, "myself" or "minhyuk", the vectors are similar, and this means that the word "myself" has the same meaning in two classes Happy and Love And the word "minhyuk" (South Korean singer) has the same meaning in two similar classes Excited and Energetic. \\
Considering "safe" between Happy and Curious, if we are sure about the accuracy of data and labeling, the most probable reason for this difference is the different contexts for each class. 

\bigskip

%------------------------------------------------
\pagebreak
\section{Tokenization}
Training the SentencePiece model with two types 'bpe' and 'unigram' on different vocabulary sizes to identify the best parameters for the tokenizer model. Each label trained separately.
\subsection{Happy}
\subsubsection{Unigram model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Happy_unigram.csv}
\end{align*} 
\subsubsection{BPE model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Happy_bpe.csv}
\end{align*} 
As the results shows, both models have the smallest percentage of UNK tokens for 50 vocabulary size.
\subsection{Sad}
\subsubsection{Unigram model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Sad_unigram.csv}
\end{align*} 
\subsubsection{BPE model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Sad_bpe.csv}
\end{align*} 
As the results shows, both models have the smallest percentage of UNK tokens for 50 vocabulary size.

\subsection{Angry}
\subsubsection{Unigram model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Angry_unigram.csv}
\end{align*} 
\subsubsection{BPE model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Angry_bpe.csv}
\end{align*} 
As the results shows, both models have the smallest percentage of UNK tokens for 50 vocabulary size.

\subsection{Fearful}
\subsubsection{Unigram model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Fearful_unigram.csv}
\end{align*} 
\subsubsection{BPE model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Fearful_bpe.csv}
\end{align*} 
As the results shows, both models have the smallest percentage of UNK tokens for 50 vocabulary size.

\subsection{Love}
\subsubsection{Unigram model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Love_unigram.csv}
\end{align*} 
\subsubsection{BPE model}
\begin{align*}
	\csvautotabular{../reports/tokenization/Love_bpe.csv}
\end{align*} 
As the results shows, both models have the smallest percentage of UNK tokens for 50 vocabulary size.

\subsection{Conclusion}
After training multiple models on different labels, I find out that the unigram model with vocabulary size 50 is a good coordination for tokenizer model. So I chose 50 as vocabulary size and the unigram model to tokenize the input.
\bigskip

%------------------------------------------------
\section{Parsing}
Trained the parser model, to identify the dependency parsing of each sentece. Here are two examples of the sentences with UAS score 86.667:\\
"you cropped the best answers" with dependency parse [(2, 1), (5, 4), (5, 3), (2, 5), (0, 2)].\\ 
To explain the result, consider (5, 4), this means (head: "answers", tail: "best") transition.\\
"yes i have reported several tweets and the profile itself" with dependency parse [(4, 3), (4, 2), (4, 1), (6, 5), (9, 8), (9, 7), (6, 9), (4, 6), (4, 10), (0, 4)]. (Will be better in next versions!)
%------------------------------------------------
\section{Language Model}
Using language model implementation \href{https://github.com/pytorch/examples}{here}, a LSTM model with 200 hidden state size and 2 layers, Trained and generated text for each label.
\subsection{Happy}
Model trained on this label with the ppl 4.64 by running 10 epochs. Sentences generated for the happy class are:\\
\input{../reports/language_model/Happy.txt}
The results seems reasonable for Happy class. Some wrong phrases occurred, for e.x "your my picture".\\
\subsection{Angry}
Model trained on this label with the ppl 1.91 by running 10 epochs. Sentences generated for the angry class are:\\
\input{../reports/language_model/Angry.txt}
Some phrases shows angry feelings well, like "i hate". But still have many grammatical and contextual errors.\\
\subsection{Sad}
Model trained on this label with the ppl 4.16 by running 10 epochs. Sentences generated for the sad class are:\\
\input{../reports/language_model/Sad.txt}
Some phrases shows sad feelings well, like "i wand your headache", but still have many grammatical and contextual errors.\\
\subsection{Love}
Model trained on this label with the ppl 39.04 by running 10 epochs. Sentences generated for the love class are:\\
\input{../reports/language_model/Love.txt}
Some phrases shows love feelings well by containing special words like "heart", "happiness" and "beautiful", but still have many grammatical and contextual errors.\\
%------------------------------------------------
\end{document}
